{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Machine Translation (Na√Øve)\n",
    "\n",
    "This notebook will cover  $Y = R ¬∑ X$ machine translation. A linear transformation of space X into Y. It's na√Øve because it assumes word inpendence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohituttamchandani/opt/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rohituttamchandani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/rohituttamchandani/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import string\n",
    "import time\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Download stopwords and tweets:\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.utils.preprocessor import TextPreprocessor\n",
    "from src.utils.embedding_translator import EmbeddingTranslator\n",
    "\n",
    "from os import getcwd\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Twitter processor in selected language:\n",
    "twitter_processor = TextPreprocessor('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "\n",
    "# 1. The word embeddings data for English and French words\n",
    "\n",
    "Write a program that translates English to French.\n",
    "\n",
    "## The data\n",
    "\n",
    "The full dataset for English embeddings is about 3.64 gigabytes, and the French\n",
    "embeddings are about 629 megabytes.\n",
    "\n",
    "Download from:\n",
    "* English embeddings from Google code archive word2vec\n",
    "[look for GoogleNews-vectors-negative300.bin.gz](https://code.google.com/archive/p/word2vec/)\n",
    "    * You'll need to unzip the file first.\n",
    "* and the French embeddings from\n",
    "[cross_lingual_text_classification](https://github.com/vjstark/crosslingual_text_classification).\n",
    "    * in the terminal, type (in one line)\n",
    "    `curl -o ./wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_translations(file):\n",
    "    dict_translation = {}\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                original_translation = f.readline().split()\n",
    "                dict_translation[original_translation[0]] = original_translation[1]\n",
    "            except:\n",
    "                print('skipped:',original_translation)\n",
    "    return dict_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "en_embeddings = KeyedVectors.load_word2vec_format(f'{sys.path[-1]}/data/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "fr_embeddings = KeyedVectors.load_word2vec_format(f'{sys.path[-1]}/data/wiki.multi.fr.vec')\n",
    "\n",
    "\n",
    "# loading the english to french dictionaries\n",
    "en_fr_train = get_dict_translations(f'{sys.path[-1]}/data/en-fr.train.txt')\n",
    "print('The length of the english to french training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict_translations(f'{sys.path[-1]}/data/en-fr.test.txt')\n",
    "print('The length of the english to french test dictionary is', len(en_fr_train))\n",
    "\n",
    "english_set = set(en_embeddings.key_to_index)\n",
    "french_set = set(fr_embeddings.key_to_index)\n",
    "en_embeddings_subset = {}\n",
    "fr_embeddings_subset = {}\n",
    "french_words = set(en_fr_train.values())\n",
    "\n",
    "for en_word in en_fr_train.keys():\n",
    "    fr_word = en_fr_train[en_word]\n",
    "    if fr_word in french_set and en_word in english_set:\n",
    "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
    "\n",
    "\n",
    "for en_word in en_fr_test.keys():\n",
    "    fr_word = en_fr_test[en_word]\n",
    "    if fr_word in french_set and en_word in english_set:\n",
    "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
    "\n",
    "pickle.dump( en_embeddings_subset, open( f\"{sys.path[-1]}/data/en_embeddings.p\", \"wb\" ) )\n",
    "pickle.dump( fr_embeddings_subset, open( f\"{sys.path[-1]}/data/fr_embeddings.p\", \"wb\" ) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the english to french training dictionary is 4070\n",
      "skipped: []\n",
      "The length of the english to french test dictionary is 4070\n"
     ]
    }
   ],
   "source": [
    "en_embeddings_subset = pickle.load(open(f\"{sys.path[-1]}/data/en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(f\"{sys.path[-1]}/data/fr_embeddings.p\", \"rb\"))\n",
    "# loading the english to french dictionaries\n",
    "en_fr_train = get_dict_translations(f'{sys.path[-1]}/data/en-fr.train.txt')\n",
    "print('The length of the english to french training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict_translations(f'{sys.path[-1]}/data/en-fr.test.txt')\n",
    "print('The length of the english to french test dictionary is', len(en_fr_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "    # X_l and Y_l are lists of the english and french word embeddings\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    # get the english words (the keys in the dictionary) and store in a set()\n",
    "    english_set = set(english_vecs.keys())\n",
    "\n",
    "    # get the french words (keys in the dictionary) and store in a set()\n",
    "    french_set = set(french_vecs.keys())\n",
    "\n",
    "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    # loop through all english, french word pairs in the english french dictionary\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "\n",
    "        # check that the french word has an embedding and that the english word has an embedding\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "\n",
    "            # get the english embedding\n",
    "            en_vec = english_vecs[en_word]\n",
    "\n",
    "            # get the french embedding\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "\n",
    "            # add the english embedding to the list\n",
    "            X_l.append(en_vec)\n",
    "\n",
    "            # add the french embedding to the list\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    # stack the vectors of X_l into a matrix X\n",
    "    X = np.vstack(X_l)\n",
    "\n",
    "    # stack the vectors of Y_l into a matrix Y\n",
    "    Y = np.vstack(Y_l)\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4024, 300) (4024, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = get_matrices(en_fr_train,fr_embeddings_subset,en_embeddings_subset)\n",
    "print(X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "# 2. Translations\n",
    "\n",
    "## 2.1 Translation as linear transformation of embeddings\n",
    "\n",
    "Given dictionaries of English and French word embeddings you will create a transformation matrix `R`\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing translation as the minimization problem\n",
    "\n",
    "Find a matrix `R` that minimizes the following equation. \n",
    "\n",
    "$$\\arg \\min _{\\mathbf{R}}\\| \\mathbf{X R} - \\mathbf{Y}\\|_{F}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual loss function\n",
    "In the real world applications, the Frobenius norm loss:\n",
    "\n",
    "$$\\| \\mathbf{XR} - \\mathbf{Y}\\|_{F}$$\n",
    "\n",
    "is often replaced by it's squared value divided by $m$:\n",
    "\n",
    "$$ \\frac{1}{m} \\|  \\mathbf{X R} - \\mathbf{Y} \\|_{F}^{2}$$\n",
    "\n",
    "where $m$ is the number of examples (rows in $\\mathbf{X}$).\n",
    "\n",
    "* The same R is found when using this loss function versus the original Frobenius norm.\n",
    "* The reason for taking the square is that it's easier to compute the gradient of the squared Frobenius (Squaring cancels the square root in the Frobenius norm formula. Because of the chain rule, we would have to do more calculations if we had a square root in our expression for summation. )\n",
    "* The reason for dividing by $m$ is that we're more interested in the average loss per embedding than the  loss for the entire training set.\n",
    "    * The loss for all training set increases with more words (training examples),\n",
    "    so taking the average helps us to track the average loss regardless of the size of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Computing the loss\n",
    "* The loss function will be squared Frobenoius norm of the difference between\n",
    "matrix and its approximation, divided by the number of training examples $m$.\n",
    "* Its formula is:\n",
    "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
    "\n",
    "where $a_{i j}$ is value in $i$th row and $j$th column of the matrix $\\mathbf{XR}-\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # diff is XR - Y\n",
    "    diff = np.matmul(X,R)-Y\n",
    "\n",
    "    # diff_squared is the element-wise square of the difference\n",
    "    diff_squared = np.square(diff)\n",
    "\n",
    "    # sum_diff_squared is the sum of the squared elements\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    # loss i the sum_diff_squard divided by the number of examples (m)\n",
    "    loss = (1./m)*sum_diff_squared\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Computing the gradient of loss in respect to transform matrix R\n",
    "\n",
    "* Calculate the gradient of the loss with respect to transform matrix `R`.\n",
    "* The gradient is a matrix that encodes how much a small change in `R`\n",
    "affect the change in the loss function.\n",
    "* The gradient gives us the direction in which we should decrease `R`\n",
    "to minimize the loss.\n",
    "* $m$ is the number of training examples (number of rows in $X$).\n",
    "* The formula for the gradient of the loss function $ùêø(ùëã,ùëå,ùëÖ)$ is:\n",
    "\n",
    "$$\\frac{d}{dR}ùêø(ùëã,ùëå,ùëÖ)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a matrix of dimension (n,n) - gradient of the loss function L for given X, Y and R.\n",
    "    '''\n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # gradient is X^T(XR - Y) * 2/m\n",
    "    gradient = (2./m)*np.dot(X.T,np.matmul(X,R)-Y)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with a fixed number of iterations\n",
    "\n",
    "Most of the time we iterate for a fixed number of training steps rather than iterating until the loss falls below a threshold.\n",
    "\n",
    "##### OPTIONAL: explanation for fixed number of iterations\n",
    "\n",
    "<p>\n",
    "<ul>\n",
    "    <li> You cannot rely on training loss getting low -- what you really want is the validation loss to go down, or validation accuracy to go up. And indeed - in some cases people train until validation accuracy reaches a threshold, or -- commonly known as \"early stopping\" -- until the validation accuracy starts to go down, which is a sign of over-fitting.\n",
    "    </li>\n",
    "    <li>\n",
    "    Why not always do \"early stopping\"? Well, mostly because well-regularized models on larger data-sets never stop improving. Especially in NLP, you can often continue training for months and the model will continue getting slightly and slightly better. This is also the reason why it's hard to just stop at a threshold -- unless there's an external customer setting the threshold, why stop, where do you put the threshold?\n",
    "    </li>\n",
    "    <li>Stopping after a certain number of steps has the advantage that you know how long your training will take - so you can keep some sanity and not train for months. You can then try to get the best performance within this time budget. Another advantage is that you can fix your learning rate schedule -- e.g., lower the learning rate at 10% before finish, and then again more at 1% before finishing. Such learning rate schedules help a lot, but are harder to do if you don't know how long you're training.\n",
    "    </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:\n",
    "1. Calculate gradient $g$ of the loss with respect to the matrix $R$.\n",
    "2. Update $R$ with the formula:\n",
    "$$R_{\\text{new}}= R_{\\text{old}}-\\alpha g$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate, which is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate\n",
    "\n",
    "* The learning rate or \"step size\" $\\alpha$ is a coefficient which decides how much we want to change $R$ in each step.\n",
    "* If we change $R$ too much, we could skip the optimum by taking too large of a step.\n",
    "* If we make only small changes to $R$, we will need many steps to reach the optimum.\n",
    "* Learning rate $\\alpha$ is used to control those changes.\n",
    "* Values of $\\alpha$ are chosen depending on the problem, and we'll use `learning_rate`$=0.0003$ as the default value for our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "    np.random.seed(129)\n",
    "\n",
    "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
    "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        # use the function that you defined to compute the gradient\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "\n",
    "        # update R by subtracting the learning rate times gradient\n",
    "        R -= learning_rate*gradient\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate transformation matrix R\n",
    "\n",
    "**NOTE:** The code cell below will take a few minutes to fully execute (~3 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 978.6459\n",
      "loss at iteration 25 is: 96.3921\n",
      "loss at iteration 50 is: 27.0718\n",
      "loss at iteration 75 is: 10.3219\n",
      "loss at iteration 100 is: 4.8282\n",
      "loss at iteration 125 is: 2.6512\n",
      "loss at iteration 150 is: 1.6687\n",
      "loss at iteration 175 is: 1.1823\n",
      "loss at iteration 200 is: 0.9248\n",
      "loss at iteration 225 is: 0.7812\n",
      "loss at iteration 250 is: 0.6980\n",
      "loss at iteration 275 is: 0.6481\n",
      "loss at iteration 300 is: 0.6174\n",
      "loss at iteration 325 is: 0.5979\n",
      "loss at iteration 350 is: 0.5854\n",
      "loss at iteration 375 is: 0.5772\n",
      "loss at iteration 400 is: 0.5718\n",
      "loss at iteration 425 is: 0.5681\n",
      "loss at iteration 450 is: 0.5655\n",
      "loss at iteration 475 is: 0.5638\n",
      "loss at iteration 500 is: 0.5625\n",
      "loss at iteration 525 is: 0.5617\n",
      "loss at iteration 550 is: 0.5611\n",
      "loss at iteration 575 is: 0.5606\n",
      "loss at iteration 600 is: 0.5603\n",
      "loss at iteration 625 is: 0.5601\n",
      "loss at iteration 650 is: 0.5600\n",
      "loss at iteration 675 is: 0.5598\n",
      "loss at iteration 700 is: 0.5598\n",
      "loss at iteration 725 is: 0.5597\n",
      "loss at iteration 750 is: 0.5597\n",
      "loss at iteration 775 is: 0.5596\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=800, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2-2\"></a>\n",
    "\n",
    "## 2.2 Testing the translation\n",
    "\n",
    "### k-Nearest neighbors algorithm\n",
    "\n",
    "[k-Nearest neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) \n",
    "* k-NN is a method which takes a vector as input and finds the other vectors in the dataset that are closest to it. \n",
    "* The 'k' is the number of \"nearest neighbors\" to find (e.g. k=2 finds the closest two neighbors).\n",
    "\n",
    "### Searching for the translation embedding\n",
    "Since we're approximating the translation function from English to French embeddings by a linear transformation matrix $\\mathbf{R}$, most of the time we won't get the exact embedding of a French word when we transform embedding $\\mathbf{e}$ of some particular English word into the French embedding space. \n",
    "* This is where $k$-NN becomes really useful! By using $1$-NN with $\\mathbf{eR}$ as input, we can search for an embedding $\\mathbf{f}$ (as a row) in the matrix $\\mathbf{Y}$ which is the closest to the transformed vector $\\mathbf{eR}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "Cosine similarity between vectors $u$ and $v$ calculated as the cosine of the angle between them.\n",
    "The formula is \n",
    "\n",
    "$$\\cos(u,v)=\\frac{u\\cdot v}{\\left\\|u\\right\\|\\left\\|v\\right\\|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Distance can be calculated as the opposite of similarity.\n",
    "$$d_{\\text{cos}}(u,v)=1-\\cos(u,v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex-05\"></a>\n",
    "\n",
    "**Exercise 05**: Complete the function `nearest_neighbor()`\n",
    "\n",
    "Inputs:\n",
    "* Vector `v`,\n",
    "* A set of possible nearest neighbors `candidates`\n",
    "* `k` nearest neighbors to find.\n",
    "* The distance metric should be based on cosine similarity.\n",
    "* Iterate over rows in `candidates`, and save the result of similarities between current row and vector `v` in a python list. Take care that similarities are in the same order as row vectors of `candidates`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = EmbeddingTranslator(en_embeddings_subset, fr_embeddings_subset, R_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chat'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your translation and compute its accuracy\n",
    "\n",
    "* Obtain an index of the closest French embedding by using\n",
    "`nearest_neighbor` (with argument `k=1`), and compare it to the index\n",
    "of the English embedding you have just transformed.\n",
    "* Calculate accuracy as $$\\text{accuracy}=\\frac{\\#(\\text{correct predictions})}{\\#(\\text{total predictions})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "\n",
    "    # The prediction is X times R\n",
    "    pred = np.matmul(X,R)\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through each row in pred (each transformed embedding)\n",
    "    for i in range(len(pred)):\n",
    "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
    "        pred_idx = translator.k_nearest_neighbor(pred[i],Y,1)\n",
    "\n",
    "        # if the index of the nearest neighbor equals the row of i... \\\n",
    "        if pred_idx == i:\n",
    "            # increment the number correct by 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
    "    accuracy = num_correct/X.shape[0]\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how is your translation mechanism working on the unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train set is 0.602\n"
     ]
    }
   ],
   "source": [
    "acc_train = test_vocabulary(X_train, Y_train, R_train)\n",
    "print(f\"accuracy on train set is {acc_train:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.602\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train) \n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC1-4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
